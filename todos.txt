I think I need to differentiate between var assignment and var declaration. It would help, especially when assigning variables from within nested blocks. Currently if there's an `x = 1` in the global scope, and in a block `x = 2`, the global scope x stays unchanged.
A related BUG: in #eval_block for anonymous blocks, if I push a scope then it breaks while loops. I think while loop's anon block evals on the current scope, so by pushing a scope, the x = 2 is declared on that scope and not over

Implement class functions. #new is reserved for inits. Also make #new() work, check #eval_block_call
d = Dog.new
◼︎ #<Instance_Construct:0x00000001006174d8>
d = Dog.new()
◼︎ Calling class functions or variables is not implemented

Cleanup Expr, after removing @short_form I replaced each instance of that to `false`.

Functional and while blocks builtin variables it, at, skip, stop. Later I'd like to add a delete keyword for arrays so you could do something like
[1, 2, 3].each { ->
	if it == 2 { remove }
}
# => [1, 3]
An alternate version, which I like. I actually like them both, I'm sure there's a use case for the remove keyword
[1, 2, 3].remove { ->
	it == 2
}

Upgrade the entire language pipeline. Currently each time I need to parse/interpret, I have to create a local instance of Lexer, then Parser, then Interpreter. Maybe there should be some class that handles the entire pipeline, and you could specify what stages you want (lex, parse, interpret). Obviously parse requires lex and interpret requires lex and parse. But this should definitely be an option.
This would also allow other options, like documenting and server stuff.
Don't just replace all places where these are locally created, do that methodically. For example, parser sometimes calls a new instance of itself, so that doesn't need an entire pipeline.

Use signature for declaration setting and lookups

Investigate parser creating a new instance of itself and having it parse output. I can't remember why I did that, maybe because Parser has @expressions and the #parse loop adds to expressions each time it runs once. So you could end up in a situation where you are mid parsing something where you have to call #parse in a nested manner, the inner #parse call will append to @expressions before the outer #parse call does, which would lead to incorrect parsing. So maybe this isn't a problem at all?

Improve error messaging. It is currently a pain in the ass to figure out error messages. It doesn't even return a location for the offending code – that would be a good place to start.

Block_Expr.compositions are just strings, I think they should be Identifier_Exprs
Parser: when eating tokens into expressions that have a :name attribute, always store the token and not just token.string. That's currently how it is, but I need the actual, especially because Token has or should have cursor position info
Generalization: Think about whether I really need Tokens stored on Asts

REPL needs to call inspect or whatever on the output so that the interpreter isn't making those transformations. That would solve my issue of trying to `!> some_instance` where it prints <Instance:#13241234> or whatever the object prints like in Ruby.
I tried having #evaluate return an inspect-like string for Instance_Construct, but that breaks interpretation because #evaluate should really be dealing with constructs to product a value, not string representations of a construct

Figure out string interpolation. See String_Literal_Expr#string=
This might be tricky. I can pull out all expressions
	"Testing `a + b * c` or `boo`".scan(/`([^`]+)`/).flatten # => ["a + b * c", "boo"]
So I think the interpreter needs to put each of these into a new lex+parse instance. then when it gets those results back, it can call #evaluate with each one to get the value.
The interpreter should also make a dictionary out of the extracted expressions. Then when each one is evaluated, store the output as its value. Later when they're being replaced into the string, it will be easy to replace even the complex expressions by string. Example of replacing:
	replacements = {
	  'name' => 'Locke',
	  'day' => 'tomorrow'
	}
	"`name`, today is `day`".gsub(/`([^`]+)`/) { replacements[$1] } # => "Locke, today is tomorrow"

Cannot escape strings. Probably should be lumped with string interpolation work.
'Test\'ing'
◼︎ Expected ''' but got ''
"Test\"ing"
◼︎ Expected '"' but got ''
\n also doesn't work.
> "Apparently not\n"does interpolation work yet? `result`""

Concatenating strings still broken for two strings.
!> 'Test ' + 'this'
"Test "this""

Operator overloading. Add new Operator_Token keyword, I need operators to be identifiable by pattern so that I can check for [Keyword_Token(operator), Operator_Token([] or other binary operator)]. Store it as a named Block_Expr aka function. The name is the operator. For Subscript should be named `[]`. So when interpreter encounters a Subscript_Expr, treat that as a Binary_Expr where left is subscript.left and the operator is dot, and the right is get_from_scope :functions, '[]'.
Any operator should be able to be overloaded, even ranges. Even `.`? That might be wild. So you can't change it on a global scope but you can on a class
Abc {
	operator . { other ->
		if other == 'new'
			!> 'called .new on `self`'
		}
	}
}
Thoughts today (without reading above thoughts): override operator with `operator` keyword. All operators have built in implementations by default but can be overloaded with `operator == { other -> }`, which gets executed on the receiver. To achieve that:
1) update parser to create named Block_Expr where name is the operator token
2) interpreter should store this under functions['=='] = Block_Construct or whatever
3) interpreter should call these functions when encountering them while parsing Binary_Exprs, since that's really the only place these operators will ever be present.

Apparently you can add members to an instance at any time by doing instance.whatever {} or instance.whatever = 'boo'. I kinda like that but it feels wrong, actually. Consider whether to keep this behavior. I just thought of a useful case for this – interpreter could have debug and release modes. In debug mode, you can add instances like this to make building the thing easier. In release mode, it would fail to interpret. I like this a lot.

Issue interpreting. I wonder what to do about this. I plan to support .0, .1, etc as a binary expression to be able to easily access arrays by index (arr.1, arr.2, etc). That would be so nice. Once operator overloading works, you could overload . on Array and have it call through to []. Something like `operator . { right -> self.[right] }`.
Abc { boo = 'boo' }.new.boo
◼︎ "boo"
Abc { boo = 'boo' }.new.boo.1
◼︎ 0.1

Implement '@' context operator, for accessing functions and variables internal to an instances, for example, @some_member. eg) all classes might have `id =;`, to access it use `@id`. That way the internal members don't pollute the instance(aka the current runtime_scope). The end result is that you can use any name for members, even internal ones. And the internal ones can be accessed only by prepending the member with @

Generalize Interpreter#evaluate when Block_Call_Expr so that Block_Expr evaluation uses the same code to call the block inline

Generalize parsing expressions between parens. I think the function call ast parses parameters between parens, so go look there. When abstracted it can be reused for tuples. Also where Parser#make_ast checks curr? '(' I'm calling parse_expression again. But I think this is where I want to use #make_comma_separated_ast
#tuples

Add support for inline conditional at end of lines because they reads nicely: `move() if xy else stop()`
Possible strategy: pop last ast and replace with new ast to make conditionals (unless while if) work at the end of an expression. Pop last expression, it becomes the when_true of a Conditional_Expr

Why are there two arrays storing ascii symbols? Ascii_Token::BINARY_OPERATORS and Ascii_Token::UNARY_OPERATORS, and in Lexer::SYMBOLS. When you change one, you have to update the operator precedence in parser, and also lexer DOUBLE_SYMBOLS

Implement, a ? b : c or is that unnecessary? I kind of like the `expr if expr else expr` syntax.

abc ?? xyz (abc if abc, otherwise xyz). This is basically how Ruby's || works. Maybe I don't need this since the interpreter uses Ruby's || when interpreting Em's ||

Should nil be an object or just a string?

Use function signature in #get_construct and #set_construct? But I'm not sure how, since Function_Call_Expr only knows a function's name.

Double check why Identifier_Expr and Identifier_Token both have identical #constant?, #object?, and #member?

Clarity issue. Identifier_Expr/Identifier_Token #member? because it's not clear. Add #variable? and #function? and make #member? return `variable? or function?`

Consider identifiers starting with a number again like 1st, 2nd, 3rd?, 4th!, etc

Parser returns Nil_Expr when it parses `nil`. Should it work differently? I'm not sure yet

In both Lexer and Parser, create some kind of error message object that functions similarly to localization. Would be cool to allow you to customize the error messages. ERRORS(:some_error) could read errors from a custom file based on the current language, like errors/en.yml or something like that. So you could define your own error messages for the language. Pointless but probably fun

Repl: make ctrl+c cancel the current input. Currently it cancels the current line on screen but preserves the input prior to cancelling. So when you start typing the next line, even though it doesn't show the previous line, the previous characters are still part of some internal state that tracks typed characters.

Consider composition with members. Parser#make_ast prevents it

Figure out how to call functions without parens like in Ruby. See Parser#make_function_call_ast
When calling func without parens, think about how to handle collisions between variable and func names. Currently the interpreter lets you call a func without parens only if it takes no arguments. If it does, it prints a message stating the block expects args.

I like how Ruby has classes for builtin types, like TrueClass, FalseClass, Integer, etc. Reading through true_class.rb, it dawned on me that there's probably some preload.rb-like file that would declare `true = TrueClass.new` or something like that. For example, I'm currently lexing a Boolean_Token when an identifier is 'true' or 'false', then parsing that into a Boolean_Literal_Expr, and finally interpreting Boolean_Literal_Expr using Boolean_Literal_Expr#to_bool.
But maybe that would be ugly? Once I add types, do I really want to type `Int` instead of `int`? I feel like the capitalization will get annoying, plus it's ugly. So maybe foundational types like int, float, array, dict, etc should all be lowercase? Idk yet

Clean up Lexer#LEX – there's a mix of methods and some inline creations right at the conditional. It would be nice to be consistent.

Think about how to "interpret" the entire program before actually interpreting, even across multiple files, such that code doesn't have to be in a specific order, so that I can reference an identifier before it is declared.

multiline strings with ```
x = ```
a multiline string!
```

Annotation comments that start with special words or characters. They should be used somehow, like when parsed, add them to some generated file or something along those lines, then they are ignored and the parser moves on.
x =; # note: blah blah
Then in `docs/notes/whatever_file_it_came_from.txt|md`, or maybe just `docs/notes.txt|md` with aggregated notes. It should work for todo, bug, and anything else I can think of later
Maybe docs should mimic the structure of the application like, docs/app/models, docs/app/controllers, etc, or whatever structure it ends up being
Along these lines, documentation comments should have their own pattern at the beginning of the string so that they can be identified as documentation. Maybe documentation should be multiline (but this looks ugly):
###
Something, something, something
###
something { -> }

bug: Assignment_Expr when passed as argument is evaluated in the scope of the function. So in this case, wtf becomes available inside the go function
go { x-> wtf }
◼︎ #<Block_Construct:0x000000010228e4b8>
go(wtf =;)
◼︎ nil
wtf
◼︎ undefined variable or function `wtf` in scope: Global
go()
◼︎ undefined variable or function `wtf` in scope: go

Add autocomplete to REPL. See https://ruby-doc.org/stdlib-2.5.1/libdoc/readline/rdoc/Readline.html#method-c-completion_proc-3D-label-Examples

Loading other files, like Ruby's require and require_relative.
@load tests/sandbox.em
How to load into a specific scope, aka named?
@load tests/sandbox.em as sandbox
The idea is that this would load the contents of the file into the current scope. It could pollute the scope, so this would allow you to put it in its own scope, like a variable assignment
Jai allows passing arguments to the loaded file, like @import 'library'(some_flag = false), then this library can evaluate based on this flag. Think about how to allow files to specify params

Parser issue — cannot set blocks as values on a dictionary. `{ x = {->} }` fails saying parser doesn't know how to parse `}`. I think this has something to do with the way that #parse_until parses until the specified token

Make print !!!> functions take comma separated expressions because it feels nicer to type a comma than a plus, because the plus requires holding shift

Come up with something better than !> !!> !!!>, mostly because that's an ugly operator

Previous scope operator `../` like `../puck.bounce` to get `puck` from the previous scope
